{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E:Implementing Decision Function of SVM RBF Kernel </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='blue'> 8F: Implementing Platt Scaling to find P(Y==1|X) </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4)          #show decimal places in numpy: https://stackoverflow.com/a/32263033/17345549\n",
    "np.set_printoptions(suppress=True)        #avoid scientific notation :https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html?highlight=formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "#generating dataset\n",
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train,crossvalidation and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, gamma=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, gamma=0.001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, gamma=0.001)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting traindata with support vector classifier\n",
    "svm=SVC(gamma=0.001, C=100)\n",
    "svm.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors=svm.support_vectors_ #getting Support vectors for our trained model\n",
    "ya=svm.dual_coef_                     #dual_coeff hold the product of y and a\n",
    "intercept=svm.intercept_              #getting the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing our custom decision function fro RBF KERNEL/GAUSSIAN KERNEL svm\n",
    "def decision_function(support_vectors,ya,X_cv,intercept,gamma):\n",
    "    predicted=[]\n",
    "    \n",
    "    for x_q in X_cv:                                                           #for each datapoint in Xcv\n",
    "        s=0\n",
    "        for idx,each_sv in enumerate(support_vectors):                         #for each support vectors\n",
    "            s+=ya[0][idx]*np.exp(-gamma*np.linalg.norm(each_sv - x_q) ** 2 )   #prediction formula\n",
    "        s=s+intercept\n",
    "        predicted=np.append(predicted,s)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make prediction on X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_Prediced_CV_y=decision_function(support_vectors,ya,X_cv,intercept,gamma=0.001)#getting our prediced y for X_cv data\n",
    "\n",
    "sklearn_predited_CV_y=svm.decision_function(X_cv)                                 #getting sklearn's svc prediction of y for X_cv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.222  -1.7992 -0.9023  3.1875 -4.5379 -1.4901 -2.2874  3.4101 -3.8576\n",
      " -2.0655 -2.2346  2.3291  0.8228 -3.207  -0.12    1.4664 -1.5481  0.4787\n",
      " -1.2393 -2.4651 -2.1121 -3.4784  0.7377 -1.4448 -4.1376  1.7319 -1.2062\n",
      "  0.69    2.2297  1.4439 -2.9107  0.3471  1.7427  1.9081 -2.0321 -2.9724\n",
      "  1.9181  1.8833 -3.2304 -3.5096  0.2469 -1.3192 -1.5229 -2.5659 -2.8968\n",
      " -2.6942 -4.0976 -2.8596 -3.3219 -1.6731  1.7599 -0.2305 -3.391  -3.0915\n",
      " -1.4975 -4.3143 -3.8562 -2.225   2.7581  1.8459 -2.7246  1.9752 -2.7957\n",
      " -2.0971 -2.6849  0.384  -3.1416 -0.2982  1.6769 -3.8401 -2.824  -2.5233\n",
      "  1.9121 -4.1093 -2.7992 -2.5886  0.3757  1.6818  1.2908  2.8399 -4.6471\n",
      "  1.1602  0.0757 -3.1137 -3.2297 -1.0368  1.8852  1.8161 -1.9506 -3.4835\n",
      " -3.4449 -0.2841 -1.1326 -3.8028  1.8205 -1.8768 -0.4398 -2.3961 -3.0015\n",
      "  2.6465 -3.5251 -2.0916  2.4988 -2.1559 -2.8897 -3.0914 -2.557   1.3397\n",
      " -0.7346 -3.2437 -2.1941 -3.2689 -2.6945 -3.511  -3.4421  1.8781 -2.032\n",
      " -2.7699 -2.8114 -2.513  -1.4655 -1.7036 -0.6792 -3.3748 -2.0048 -2.9845\n",
      " -2.6355 -4.1013  1.5971 -2.9896 -3.6019 -3.4314  2.5146  2.2997 -3.2041\n",
      " -4.2198  0.3745  1.4755 -3.2519 -3.5609 -2.0034 -1.4041 -4.0253 -0.5764\n",
      "  2.3247 -2.069  -1.3818 -3.1618  1.195  -4.5364 -1.901   1.2401 -2.1333\n",
      " -2.7867 -2.5757 -1.791  -0.6647 -2.9999 -2.1261 -3.5569 -2.5921 -2.8254\n",
      "  0.0712 -4.1764  2.1762  2.0637  1.602  -2.6778 -3.2154 -1.5146 -2.781\n",
      "  2.3185 -3.183  -2.8408 -4.2473 -1.5981 -2.8183  1.1038 -0.6346 -0.3124\n",
      " -3.1454 -1.3227 -0.1631  0.2657 -1.3542 -0.5199 -2.7951 -1.6272  2.537\n",
      "  1.4959 -0.3967 -1.6563 -3.0699 -2.1934 -0.0145  2.9245 -3.6897  1.5579\n",
      " -2.9628  3.0546 -3.6551 -0.522  -2.6733  0.2803 -2.6628  2.203  -2.6339\n",
      "  1.844  -3.9508 -3.1786  2.4395 -4.7023  2.2579 -0.9185 -4.1305 -2.4905\n",
      " -2.1466  1.6953  1.005  -0.7834 -3.4857 -2.8356 -2.3557  1.2261 -1.0222\n",
      " -4.3111 -3.4278 -1.472   1.756   1.4446  0.844  -2.6603  1.2664 -4.1247\n",
      " -3.0313 -2.874  -2.1465  1.5501  1.0361 -3.2029 -3.1351  1.5816 -4.2952\n",
      " -2.2257 -0.6723 -4.0559  2.1321 -2.6939 -1.7484 -4.279   2.0047 -2.2405\n",
      " -2.1419 -2.3032  0.1919 -3.8222 -2.1924  1.1839 -2.0977  1.8653 -2.3018\n",
      "  1.8686 -0.8824 -0.2203 -2.1343 -0.8898  2.1312 -2.2365  1.7552  2.4336\n",
      " -3.8103 -0.4199 -2.5978  0.2984  1.771  -2.2176  0.5283 -2.7627 -0.0069\n",
      " -2.255   1.7487  1.7009  0.3361 -3.6712 -1.5279 -3.9171  2.4845  1.4867\n",
      " -2.7503 -0.6015  0.0851  0.0524 -2.4256  1.5571 -1.6746 -2.0599 -3.0281\n",
      "  0.6375  1.6982  0.7776 -2.8142 -1.9996 -1.6753 -2.5742  1.8524 -3.2693\n",
      " -2.1513  2.2548 -3.8681 -1.5578  0.9516 -0.4899 -3.3347  1.8679  2.2394\n",
      " -3.3072 -3.7137  1.8797  1.8324 -2.9842 -2.9609 -2.5431 -3.1029 -2.0078\n",
      "  2.274   2.5221 -3.7932 -2.8779 -3.0281 -0.0129 -1.0842 -1.9487 -2.7209\n",
      "  1.7676 -3.7838  2.6546  1.9604 -1.067   0.8712 -3.1164 -3.9821  1.856\n",
      " -2.6313 -3.411  -1.5169 -2.2839 -2.5673 -2.7273 -2.3395 -1.6654 -1.7361\n",
      " -1.442  -1.0559 -2.3965  1.3592  2.6481  0.6387 -2.5402 -3.4016 -2.0493\n",
      "  1.2744 -2.5745 -1.7345 -1.6392 -1.245  -1.6222 -2.3504 -4.3668  0.3568\n",
      " -2.2079 -1.6851 -3.9409 -3.5576 -1.1321 -3.0381 -3.3646  1.4333  1.2637\n",
      " -0.5394 -2.5795 -2.7599 -1.2923  1.5682 -0.5689 -2.0673 -0.4315 -2.8037\n",
      " -2.4082  1.7932  1.2734 -2.6254 -2.9643 -1.4145 -3.8933 -2.4151 -1.9035\n",
      "  0.0512 -1.6581 -1.4918 -1.8684 -1.6352 -2.8779 -3.1716 -3.1871 -4.9652\n",
      " -1.6116 -1.9989  0.15   -0.8654 -2.9723 -1.8682 -0.389  -2.7411  1.6019\n",
      " -0.7427 -3.0229  2.008  -3.2858 -3.1957 -2.2597 -2.6416  2.3572 -0.663\n",
      "  2.0256 -0.4031  2.5314 -2.6132  2.798   2.0031  0.6173 -2.0692 -2.4396\n",
      " -2.9982  2.8388 -1.7564 -1.7187 -0.4548 -2.7675 -0.4227 -4.068  -1.192\n",
      " -2.3011 -3.8983  1.4115 -3.1652 -2.8637 -2.2706 -1.9867 -2.4922 -4.388\n",
      " -0.704  -2.3401 -3.6381 -2.6227  1.2205 -2.9582 -1.4136 -1.7916 -3.2861\n",
      "  1.0826 -1.7047 -2.5907  1.8093 -2.7829  2.2036 -1.5839  1.5112 -0.6892\n",
      " -3.029  -3.4714 -2.4027 -2.7983 -3.1415  1.7268 -2.954  -2.3567 -0.802\n",
      " -2.3962 -1.7909 -2.6407 -1.8139 -4.694   1.45   -3.1441 -3.6906 -2.3742\n",
      " -2.7573 -0.4627 -0.3497  1.9298 -2.5691  1.8186 -1.7573  3.1568 -1.9115\n",
      " -2.6819 -1.8543  1.6624  1.7724 -3.1112 -3.0132 -1.3637  3.12   -3.4656\n",
      " -2.8404 -1.6543 -0.9592  1.8145 -1.5425 -2.298  -0.9687 -2.3947 -1.6685\n",
      " -2.6932 -2.2897 -2.4571 -2.7881 -0.4701  1.0402 -2.7338 -4.8531 -2.9987\n",
      " -1.6767 -0.0475 -3.2101 -1.8459  4.0565 -1.4812  1.0702  0.755  -1.0864\n",
      " -3.5935 -1.6192 -0.3625 -4.1628 -1.8299 -0.3542 -0.798  -0.7395 -3.338\n",
      " -0.8801 -2.9923 -1.5419  1.7535 -1.92   -0.3125 -3.8584  1.7004  2.2209\n",
      " -3.1981 -0.1458 -2.5057 -0.1519 -1.239  -2.5701 -3.097  -2.7102 -0.6211\n",
      "  0.5423 -2.6837  2.8308  2.6029 -3.3669  2.1067  2.2447 -0.4484  1.9805\n",
      " -1.631  -2.3141 -1.8582 -1.937  -0.9303  2.6228 -3.1649  1.8924 -0.9183\n",
      " -2.11    1.3451 -3.7601 -3.4074 -0.0156 -3.5195  2.4136 -2.7698  1.9249\n",
      " -2.5026 -3.5621 -2.1927 -2.7951 -1.5356  0.9485  1.9266 -4.1005 -0.2766\n",
      " -1.2779  1.7821 -0.3973  1.3181 -1.7402 -2.4951 -3.4025  1.936   1.5183\n",
      " -2.8037 -1.0456  1.5218  1.9312 -2.8762 -4.7111 -3.492  -3.6729 -3.0448\n",
      " -3.0412 -2.9855 -2.7897 -3.8375 -0.1487 -2.1028 -2.0528 -1.9148  1.7893\n",
      " -3.0315 -2.7603 -2.2073  1.5443 -4.2371 -3.2701 -2.7776 -2.454  -0.8085\n",
      " -3.1925 -4.9173 -2.165  -2.3124 -3.7819 -4.714   0.0811  2.1546 -1.7614\n",
      " -2.0068 -2.1529  1.3878 -4.3047 -4.7473 -4.4927 -3.7292  3.3038 -2.8815\n",
      " -2.9772 -3.48   -2.8198 -1.8517 -0.1129 -3.0642 -2.9501 -0.9095 -2.915\n",
      "  2.8249 -2.4715 -0.9745  1.6687 -2.94    1.8667 -3.3327 -0.3408  1.7947\n",
      "  1.3551 -2.5592  1.0455 -2.5481  4.8203 -0.8127 -2.5829  0.8295 -1.4454\n",
      " -0.6682 -2.6178 -0.7079  2.242  -2.2157  0.8022  1.3482 -4.1378 -2.9419\n",
      " -0.3768  2.0849 -3.0063 -2.2797 -0.1674 -0.6878 -2.321  -1.7422 -2.8782\n",
      " -3.8892 -3.6605 -2.0609  1.1329  1.0783  1.9869 -1.1007 -4.7505  0.408\n",
      " -2.5183 -2.472  -0.8586 -2.7205 -2.3128 -2.9791 -1.7763 -3.3662 -4.5643\n",
      "  1.4686 -2.7412 -2.5054 -1.1265  1.0685  2.2832  1.9393 -2.1051 -3.3667\n",
      "  2.1167  2.2726 -2.3745 -1.2447 -2.5107 -0.6364 -3.4637  2.1383 -1.4708\n",
      " -1.1734 -2.4587  1.6839  1.5021 -1.5463 -3.4261 -0.0489 -1.7902 -1.9571\n",
      "  1.7749  1.962  -1.4137 -2.4346 -1.0586  0.9562  0.2755 -0.8509  0.8894\n",
      " -1.715   0.4441  1.7828  1.832  -0.7307  1.3395 -2.6698  0.9253 -0.8094\n",
      " -3.1328 -1.6998 -2.4394 -2.5145  0.3762  1.7375 -0.5933 -1.8073  1.3937\n",
      " -3.1781 -0.5387 -2.2156 -3.645  -3.0676 -3.5532 -3.9194 -3.3894  0.6434\n",
      " -3.1443  0.9772  2.0352 -3.4692 -2.7785 -3.9006 -3.3739 -0.4673 -1.1736\n",
      "  0.6579  2.3234  1.8798  1.6464 -2.8305 -2.2391 -4.3131  1.1972 -0.8368\n",
      " -3.2438 -3.2553 -2.7321 -3.3334 -2.7444  1.1964 -3.278   2.5633  1.9745\n",
      "  1.1232 -1.6761  2.4151 -2.4548 -3.5084  1.2577 -2.6372  0.8637 -2.4713\n",
      " -2.5356 -2.4959 -0.3112 -2.6094  2.2451  0.9049 -4.8615  0.0483 -1.2838\n",
      "  2.7977 -2.6646  1.6677 -2.646  -3.6834  1.69   -5.0351 -3.8179  0.6601\n",
      " -4.265  -2.8688  1.8331 -1.4423  1.8874  1.0085 -2.3079 -3.1319 -4.996\n",
      " -2.6749 -3.7967 -2.8447 -1.7168 -3.0345 -2.6747 -4.6932  1.7588  2.1489\n",
      " -1.5459 -3.9276 -3.9285 -0.5484 -2.5785 -2.4084  2.074   1.4781  0.8221\n",
      "  1.5589 -2.4502 -4.1976 -2.7727  1.9575 -2.0462 -3.1746 -1.1688  1.3781\n",
      " -2.4207 -4.2016 -1.403  -0.1656 -2.4872  2.5407 -2.6566 -4.4069 -1.8488\n",
      "  1.3505 -0.0513  0.7861  2.0678 -3.7497  0.9529  1.599  -2.5602  0.6003\n",
      " -1.1746 -3.3452 -3.3103 -2.6039  1.3995 -3.6947  2.5795 -3.3437 -2.0708\n",
      "  1.5554 -0.9638 -2.1911 -2.4955 -1.4609 -3.3    -1.9788 -2.7395 -1.9146\n",
      " -3.8018 -3.025  -1.3653 -2.8552 -1.6852 -2.5016 -3.5579 -1.3666  1.9462\n",
      "  0.3892 -2.1104  1.2755 -3.3966 -1.855  -1.289  -2.7598  1.2055  0.4611\n",
      "  1.5674 -0.365  -1.8332 -1.8773 -0.6668 -1.3766 -2.437  -1.3709 -2.181\n",
      " -2.0334 -0.9703 -1.7006  1.675  -3.4501 -1.2711 -2.7095 -2.1513  1.4543\n",
      "  1.8292 -2.6727  1.5964 -2.2616 -4.1383  0.8625  2.4104  0.6191  2.6459\n",
      "  1.06    3.262  -1.9432  1.8979 -2.7085 -1.7413  0.0454 -3.2582 -1.6394\n",
      " -2.7705 -1.976  -1.2297  0.4054  1.9063  1.8125  1.7247  0.3656 -0.1062\n",
      "  1.1743 -3.5764  1.6252 -3.2015 -1.5781  1.881   2.4677  2.2285  2.0461\n",
      " -0.4401 -0.3963 -4.7863 -2.1999  1.9054  1.6178 -3.0744  2.1246 -1.789\n",
      " -0.6682 -3.5428  2.6236 -1.6884 -2.0043  2.6518  1.5186  1.6449 -1.7842\n",
      "  0.1011  3.1554 -2.6808 -2.9852 -2.1893 -3.1859 -3.1306 -4.0279 -2.5151\n",
      " -2.6637]\n"
     ]
    }
   ],
   "source": [
    "#print to check whether our implementation works as good as sklearns implementation\n",
    "print(our_Prediced_CV_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.222  -1.7992 -0.9023  3.1875 -4.5379 -1.4901 -2.2874  3.4101 -3.8576\n",
      " -2.0655 -2.2346  2.3291  0.8228 -3.207  -0.12    1.4664 -1.5481  0.4787\n",
      " -1.2393 -2.4651 -2.1121 -3.4784  0.7377 -1.4448 -4.1376  1.7319 -1.2062\n",
      "  0.69    2.2297  1.4439 -2.9107  0.3471  1.7427  1.9081 -2.0321 -2.9724\n",
      "  1.9181  1.8833 -3.2304 -3.5096  0.2469 -1.3192 -1.5229 -2.5659 -2.8968\n",
      " -2.6942 -4.0976 -2.8596 -3.3219 -1.6731  1.7599 -0.2305 -3.391  -3.0915\n",
      " -1.4975 -4.3143 -3.8562 -2.225   2.7581  1.8459 -2.7246  1.9752 -2.7957\n",
      " -2.0971 -2.6849  0.384  -3.1416 -0.2982  1.6769 -3.8401 -2.824  -2.5233\n",
      "  1.9121 -4.1093 -2.7992 -2.5886  0.3757  1.6818  1.2908  2.8399 -4.6471\n",
      "  1.1602  0.0757 -3.1137 -3.2297 -1.0368  1.8852  1.8161 -1.9506 -3.4835\n",
      " -3.4449 -0.2841 -1.1326 -3.8028  1.8205 -1.8768 -0.4398 -2.3961 -3.0015\n",
      "  2.6465 -3.5251 -2.0916  2.4988 -2.1559 -2.8897 -3.0914 -2.557   1.3397\n",
      " -0.7346 -3.2437 -2.1941 -3.2689 -2.6945 -3.511  -3.4421  1.8781 -2.032\n",
      " -2.7699 -2.8114 -2.513  -1.4655 -1.7036 -0.6792 -3.3748 -2.0048 -2.9845\n",
      " -2.6355 -4.1013  1.5971 -2.9896 -3.6019 -3.4314  2.5146  2.2997 -3.2041\n",
      " -4.2198  0.3745  1.4755 -3.2519 -3.5609 -2.0034 -1.4041 -4.0253 -0.5764\n",
      "  2.3247 -2.069  -1.3818 -3.1618  1.195  -4.5364 -1.901   1.2401 -2.1333\n",
      " -2.7867 -2.5757 -1.791  -0.6647 -2.9999 -2.1261 -3.5569 -2.5921 -2.8254\n",
      "  0.0712 -4.1764  2.1762  2.0637  1.602  -2.6778 -3.2154 -1.5146 -2.781\n",
      "  2.3185 -3.183  -2.8408 -4.2473 -1.5981 -2.8183  1.1038 -0.6346 -0.3124\n",
      " -3.1454 -1.3227 -0.1631  0.2657 -1.3542 -0.5199 -2.7951 -1.6272  2.537\n",
      "  1.4959 -0.3967 -1.6563 -3.0699 -2.1934 -0.0145  2.9245 -3.6897  1.5579\n",
      " -2.9628  3.0546 -3.6551 -0.522  -2.6733  0.2803 -2.6628  2.203  -2.6339\n",
      "  1.844  -3.9508 -3.1786  2.4395 -4.7023  2.2579 -0.9185 -4.1305 -2.4905\n",
      " -2.1466  1.6953  1.005  -0.7834 -3.4857 -2.8356 -2.3557  1.2261 -1.0222\n",
      " -4.3111 -3.4278 -1.472   1.756   1.4446  0.844  -2.6603  1.2664 -4.1247\n",
      " -3.0313 -2.874  -2.1465  1.5501  1.0361 -3.2029 -3.1351  1.5816 -4.2952\n",
      " -2.2257 -0.6723 -4.0559  2.1321 -2.6939 -1.7484 -4.279   2.0047 -2.2405\n",
      " -2.1419 -2.3032  0.1919 -3.8222 -2.1924  1.1839 -2.0977  1.8653 -2.3018\n",
      "  1.8686 -0.8824 -0.2203 -2.1343 -0.8898  2.1312 -2.2365  1.7552  2.4336\n",
      " -3.8103 -0.4199 -2.5978  0.2984  1.771  -2.2176  0.5283 -2.7627 -0.0069\n",
      " -2.255   1.7487  1.7009  0.3361 -3.6712 -1.5279 -3.9171  2.4845  1.4867\n",
      " -2.7503 -0.6015  0.0851  0.0524 -2.4256  1.5571 -1.6746 -2.0599 -3.0281\n",
      "  0.6375  1.6982  0.7776 -2.8142 -1.9996 -1.6753 -2.5742  1.8524 -3.2693\n",
      " -2.1513  2.2548 -3.8681 -1.5578  0.9516 -0.4899 -3.3347  1.8679  2.2394\n",
      " -3.3072 -3.7137  1.8797  1.8324 -2.9842 -2.9609 -2.5431 -3.1029 -2.0078\n",
      "  2.274   2.5221 -3.7932 -2.8779 -3.0281 -0.0129 -1.0842 -1.9487 -2.7209\n",
      "  1.7676 -3.7838  2.6546  1.9604 -1.067   0.8712 -3.1164 -3.9821  1.856\n",
      " -2.6313 -3.411  -1.5169 -2.2839 -2.5673 -2.7273 -2.3395 -1.6654 -1.7361\n",
      " -1.442  -1.0559 -2.3965  1.3592  2.6481  0.6387 -2.5402 -3.4016 -2.0493\n",
      "  1.2744 -2.5745 -1.7345 -1.6392 -1.245  -1.6222 -2.3504 -4.3668  0.3568\n",
      " -2.2079 -1.6851 -3.9409 -3.5576 -1.1321 -3.0381 -3.3646  1.4333  1.2637\n",
      " -0.5394 -2.5795 -2.7599 -1.2923  1.5682 -0.5689 -2.0673 -0.4315 -2.8037\n",
      " -2.4082  1.7932  1.2734 -2.6254 -2.9643 -1.4145 -3.8933 -2.4151 -1.9035\n",
      "  0.0512 -1.6581 -1.4918 -1.8684 -1.6352 -2.8779 -3.1716 -3.1871 -4.9652\n",
      " -1.6116 -1.9989  0.15   -0.8654 -2.9723 -1.8682 -0.389  -2.7411  1.6019\n",
      " -0.7427 -3.0229  2.008  -3.2858 -3.1957 -2.2597 -2.6416  2.3572 -0.663\n",
      "  2.0256 -0.4031  2.5314 -2.6132  2.798   2.0031  0.6173 -2.0692 -2.4396\n",
      " -2.9982  2.8388 -1.7564 -1.7187 -0.4548 -2.7675 -0.4227 -4.068  -1.192\n",
      " -2.3011 -3.8983  1.4115 -3.1652 -2.8637 -2.2706 -1.9867 -2.4922 -4.388\n",
      " -0.704  -2.3401 -3.6381 -2.6227  1.2205 -2.9582 -1.4136 -1.7916 -3.2861\n",
      "  1.0826 -1.7047 -2.5907  1.8093 -2.7829  2.2036 -1.5839  1.5112 -0.6892\n",
      " -3.029  -3.4714 -2.4027 -2.7983 -3.1415  1.7268 -2.954  -2.3567 -0.802\n",
      " -2.3962 -1.7909 -2.6407 -1.8139 -4.694   1.45   -3.1441 -3.6906 -2.3742\n",
      " -2.7573 -0.4627 -0.3497  1.9298 -2.5691  1.8186 -1.7573  3.1568 -1.9115\n",
      " -2.6819 -1.8543  1.6624  1.7724 -3.1112 -3.0132 -1.3637  3.12   -3.4656\n",
      " -2.8404 -1.6543 -0.9592  1.8145 -1.5425 -2.298  -0.9687 -2.3947 -1.6685\n",
      " -2.6932 -2.2897 -2.4571 -2.7881 -0.4701  1.0402 -2.7338 -4.8531 -2.9987\n",
      " -1.6767 -0.0475 -3.2101 -1.8459  4.0565 -1.4812  1.0702  0.755  -1.0864\n",
      " -3.5935 -1.6192 -0.3625 -4.1628 -1.8299 -0.3542 -0.798  -0.7395 -3.338\n",
      " -0.8801 -2.9923 -1.5419  1.7535 -1.92   -0.3125 -3.8584  1.7004  2.2209\n",
      " -3.1981 -0.1458 -2.5057 -0.1519 -1.239  -2.5701 -3.097  -2.7102 -0.6211\n",
      "  0.5423 -2.6837  2.8308  2.6029 -3.3669  2.1067  2.2447 -0.4484  1.9805\n",
      " -1.631  -2.3141 -1.8582 -1.937  -0.9303  2.6228 -3.1649  1.8924 -0.9183\n",
      " -2.11    1.3451 -3.7601 -3.4074 -0.0156 -3.5195  2.4136 -2.7698  1.9249\n",
      " -2.5026 -3.5621 -2.1927 -2.7951 -1.5356  0.9485  1.9266 -4.1005 -0.2766\n",
      " -1.2779  1.7821 -0.3973  1.3181 -1.7402 -2.4951 -3.4025  1.936   1.5183\n",
      " -2.8037 -1.0456  1.5218  1.9312 -2.8762 -4.7111 -3.492  -3.6729 -3.0448\n",
      " -3.0412 -2.9855 -2.7897 -3.8375 -0.1487 -2.1028 -2.0528 -1.9148  1.7893\n",
      " -3.0315 -2.7603 -2.2073  1.5443 -4.2371 -3.2701 -2.7776 -2.454  -0.8085\n",
      " -3.1925 -4.9173 -2.165  -2.3124 -3.7819 -4.714   0.0811  2.1546 -1.7614\n",
      " -2.0068 -2.1529  1.3878 -4.3047 -4.7473 -4.4927 -3.7292  3.3038 -2.8815\n",
      " -2.9772 -3.48   -2.8198 -1.8517 -0.1129 -3.0642 -2.9501 -0.9095 -2.915\n",
      "  2.8249 -2.4715 -0.9745  1.6687 -2.94    1.8667 -3.3327 -0.3408  1.7947\n",
      "  1.3551 -2.5592  1.0455 -2.5481  4.8203 -0.8127 -2.5829  0.8295 -1.4454\n",
      " -0.6682 -2.6178 -0.7079  2.242  -2.2157  0.8022  1.3482 -4.1378 -2.9419\n",
      " -0.3768  2.0849 -3.0063 -2.2797 -0.1674 -0.6878 -2.321  -1.7422 -2.8782\n",
      " -3.8892 -3.6605 -2.0609  1.1329  1.0783  1.9869 -1.1007 -4.7505  0.408\n",
      " -2.5183 -2.472  -0.8586 -2.7205 -2.3128 -2.9791 -1.7763 -3.3662 -4.5643\n",
      "  1.4686 -2.7412 -2.5054 -1.1265  1.0685  2.2832  1.9393 -2.1051 -3.3667\n",
      "  2.1167  2.2726 -2.3745 -1.2447 -2.5107 -0.6364 -3.4637  2.1383 -1.4708\n",
      " -1.1734 -2.4587  1.6839  1.5021 -1.5463 -3.4261 -0.0489 -1.7902 -1.9571\n",
      "  1.7749  1.962  -1.4137 -2.4346 -1.0586  0.9562  0.2755 -0.8509  0.8894\n",
      " -1.715   0.4441  1.7828  1.832  -0.7307  1.3395 -2.6698  0.9253 -0.8094\n",
      " -3.1328 -1.6998 -2.4394 -2.5145  0.3762  1.7375 -0.5933 -1.8073  1.3937\n",
      " -3.1781 -0.5387 -2.2156 -3.645  -3.0676 -3.5532 -3.9194 -3.3894  0.6434\n",
      " -3.1443  0.9772  2.0352 -3.4692 -2.7785 -3.9006 -3.3739 -0.4673 -1.1736\n",
      "  0.6579  2.3234  1.8798  1.6464 -2.8305 -2.2391 -4.3131  1.1972 -0.8368\n",
      " -3.2438 -3.2553 -2.7321 -3.3334 -2.7444  1.1964 -3.278   2.5633  1.9745\n",
      "  1.1232 -1.6761  2.4151 -2.4548 -3.5084  1.2577 -2.6372  0.8637 -2.4713\n",
      " -2.5356 -2.4959 -0.3112 -2.6094  2.2451  0.9049 -4.8615  0.0483 -1.2838\n",
      "  2.7977 -2.6646  1.6677 -2.646  -3.6834  1.69   -5.0351 -3.8179  0.6601\n",
      " -4.265  -2.8688  1.8331 -1.4423  1.8874  1.0085 -2.3079 -3.1319 -4.996\n",
      " -2.6749 -3.7967 -2.8447 -1.7168 -3.0345 -2.6747 -4.6932  1.7588  2.1489\n",
      " -1.5459 -3.9276 -3.9285 -0.5484 -2.5785 -2.4084  2.074   1.4781  0.8221\n",
      "  1.5589 -2.4502 -4.1976 -2.7727  1.9575 -2.0462 -3.1746 -1.1688  1.3781\n",
      " -2.4207 -4.2016 -1.403  -0.1656 -2.4872  2.5407 -2.6566 -4.4069 -1.8488\n",
      "  1.3505 -0.0513  0.7861  2.0678 -3.7497  0.9529  1.599  -2.5602  0.6003\n",
      " -1.1746 -3.3452 -3.3103 -2.6039  1.3995 -3.6947  2.5795 -3.3437 -2.0708\n",
      "  1.5554 -0.9638 -2.1911 -2.4955 -1.4609 -3.3    -1.9788 -2.7395 -1.9146\n",
      " -3.8018 -3.025  -1.3653 -2.8552 -1.6852 -2.5016 -3.5579 -1.3666  1.9462\n",
      "  0.3892 -2.1104  1.2755 -3.3966 -1.855  -1.289  -2.7598  1.2055  0.4611\n",
      "  1.5674 -0.365  -1.8332 -1.8773 -0.6668 -1.3766 -2.437  -1.3709 -2.181\n",
      " -2.0334 -0.9703 -1.7006  1.675  -3.4501 -1.2711 -2.7095 -2.1513  1.4543\n",
      "  1.8292 -2.6727  1.5964 -2.2616 -4.1383  0.8625  2.4104  0.6191  2.6459\n",
      "  1.06    3.262  -1.9432  1.8979 -2.7085 -1.7413  0.0454 -3.2582 -1.6394\n",
      " -2.7705 -1.976  -1.2297  0.4054  1.9063  1.8125  1.7247  0.3656 -0.1062\n",
      "  1.1743 -3.5764  1.6252 -3.2015 -1.5781  1.881   2.4677  2.2285  2.0461\n",
      " -0.4401 -0.3963 -4.7863 -2.1999  1.9054  1.6178 -3.0744  2.1246 -1.789\n",
      " -0.6682 -3.5428  2.6236 -1.6884 -2.0043  2.6518  1.5186  1.6449 -1.7842\n",
      "  0.1011  3.1554 -2.6808 -2.9852 -2.1893 -3.1859 -3.1306 -4.0279 -2.5151\n",
      " -2.6637]\n"
     ]
    }
   ],
   "source": [
    "print(sklearn_predited_CV_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting y_cv to the platt scaling's required probabilily values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998898678414097\n"
     ]
    }
   ],
   "source": [
    "N_pos=np.count_nonzero(y_train)    #count number of positive points in y_train data\n",
    "N_neg=y_train.shape[0]-N_pos        #count number of negative points in y_train data\n",
    "y_pos=(N_pos+1)/(N_pos+2)           #calculate y+ as mentioned in the formula for platt scaling\n",
    "y_neg=1/(N_neg+2)                   #calculate y- as mentioned in the formula for platt scaling\n",
    "print(y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_y_cv=np.where(y_cv==1,y_pos,y_neg)   #replace element in numpy: https://numpy.org/doc/stable/reference/generated/numpy.where.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([0.0005, 0.9989]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sklearn's SGD algorithm doesn't support the real valued labels...so we can check that it throws error here....so use custom SGD function\u001b[39;00m\n\u001b[0;32m      3\u001b[0m lr\u001b[38;5;241m=\u001b[39mSGDClassifier(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,eta0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mour_Prediced_CV_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodified_y_cv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:890\u001b[0m, in \u001b[0;36mBaseSGDClassifier.fit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, coef_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, intercept_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit linear model with Stochastic Gradient Descent.\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m        Returns an instance of self.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:686\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m--> 686\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m    704\u001b[0m ):\n\u001b[0;32m    705\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum number of iteration reached before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvergence. Consider increasing max_iter to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimprove the fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    709\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    710\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:593\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    581\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    582\u001b[0m     X,\n\u001b[0;32m    583\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    589\u001b[0m )\n\u001b[0;32m    591\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 593\u001b[0m \u001b[43m_check_partial_fit_first_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Allocate datastructures from input arguments\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\multiclass.py:368\u001b[0m, in \u001b[0;36m_check_partial_fit_first_call\u001b[1;34m(clf, classes)\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    362\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`classes=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` is not the same as on last call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto partial_fit, was: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (classes, clf\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m    364\u001b[0m             )\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;66;03m# This is the first call to partial_fit\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m         clf\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# classes is None and clf.classes_ has already previously been set:\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# nothing to do\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\multiclass.py:103\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    101\u001b[0m _unique_labels \u001b[38;5;241m=\u001b[39m _FN_UNIQUE_LABELS\u001b[38;5;241m.\u001b[39mget(label_type, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _unique_labels:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(ys))\n\u001b[0;32m    105\u001b[0m ys_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([0.0005, 0.9989]),)"
     ]
    }
   ],
   "source": [
    "# Sklearn's SGD algorithm doesn't support the real valued labels...so we can check that it throws error here....so use custom SGD function\n",
    "\n",
    "lr=SGDClassifier(loss=\"log_loss\",alpha=0.001,eta0=0.001)\n",
    "lr.fit(our_Prediced_CV_y.reshape(-1, 1),modified_y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(row_vector):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    # zeros_like function to initialize zero: https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    w=np.zeros_like(row_vector)\n",
    "    b=0      #initializing bias to zero\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    if z >= 0:                          #to avoid overflow problem : referene taken from: https://developer.ibm.com/articles/implementing-logistic-regression-from-scratch-in-python/\n",
    "        z = np.exp(-z)            \n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = np.exp(z)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    #while dealing with numpy arrays you can use vectorized operations for quicker calculations as compared to using loops\n",
    "    #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "    #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/\n",
    "\n",
    "    loss =-np.mean(y_true*(np.log10(y_pred+1e-9)) + (1-y_true)*np.log10(1-y_pred+1e-9))   #to avoid division by zero error add 1e-9:reference taken from- #https://developer.ibm.com/articles/implementing-logistic-regression-from-scratch-in-python/\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw = x*(y-sigmoid(np.dot(w.T,x)+b))+((alpha/N)*w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_db(x,y,w,b):\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "     db = y-sigmoid(np.dot(w,x)+b)   \n",
    "     return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function used to compute predicted_y given the dataset X\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict=np.append(predict,sigmoid(z))\n",
    "    return np.array(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_cv,y_cv,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    epoch_tracker=[]\n",
    "    train_loss = []\n",
    "    w,b = initialize_weights(X_cv[0]) # Initialize the weights\n",
    "    #code to perform SGD\n",
    "    for i in range(epochs):                           # for every epoch\n",
    "        for idx,x in enumerate(X_cv) :             # for every data point(X_train,y_train)\n",
    "            dw=gradient_dw(x,y_cv[idx],w,b,alpha,len(X_cv))  #computing gradient w.r.to w \n",
    "            db=gradient_db(x,y_cv[idx],w,b)        #computing gradient w.r.to b \n",
    "            w+=eta0*dw                                #update w  #Here eta0 is learning rate\n",
    "            b+=eta0*db                                #update b\n",
    "            \n",
    "        y_pred_tr=pred(w,b,X_cv)           #preding y for the given x_train using logistic function\n",
    "        tr_loss=logloss(y_cv,y_pred_tr)    #calculating log loss for train datapoints\n",
    "        train_loss.append(tr_loss)\n",
    "        epoch_tracker.append(i)\n",
    "\n",
    "        \n",
    "        if i>2:\n",
    "            if abs(train_loss[-1]-train_loss[-2]) <= 1e-3:     #if the first 3 decimal places of train loss does not change for the consequent epoch,stop the training process since data is fitted to model very well\n",
    "                break\n",
    "                \n",
    "    plt.plot(epoch_tracker,train_loss)  #plot to check how trainloss reducing as number of epochs increases\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Train loss\")\n",
    "    plt.title(\"Train loss vs number of epochs\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "    return w,b,train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=0.001          #taking hyperparameter(but hyperparameter is not yet tuned)\n",
    "eta0=0.001\n",
    "epochs=30\n",
    "\n",
    "#fitting the calibation model with predicted y_cv and actual y_cv \n",
    "w,b,train_loss=train(our_Prediced_CV_y.reshape(-1, 1),modified_y_cv,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)          #shape of weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform X_test data using SVM and calibrated model sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data by\n",
    "#1)first predicting  the uncalibated prob of X_test by passing it through decision function of svm classifier\n",
    "#2)then passing the predicted uncalibated prob of X_test through pred function of calibation(log.reg) model to get calibrated prediction on X_test\n",
    "\n",
    "#SVM\n",
    "non_calibrated_y_pred_test = decision_function(support_vectors,ya,X_test,intercept,gamma=0.001) #1) step number 1\n",
    "\n",
    "#Calibration(sigmoidal)\n",
    "calibrated_y_pred_test = pred(w,b,non_calibrated_y_pred_test) # step numeber 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.6036 -1.5517 -3.3686 -2.7721 -0.634  -0.6123 -1.1856 -3.5855 -1.8323\n",
      "  1.46    1.7033 -1.6195 -3.1906  1.8562  1.3528 -1.8838  2.5774  1.5775\n",
      "  1.4923 -1.0626  2.1438 -1.1047 -2.497   2.5943  0.9765 -2.9792 -1.6881\n",
      " -3.008  -1.6357 -0.5173 -2.9984 -1.5541 -0.8679  1.9308  0.2925  1.4182\n",
      "  1.8559  1.8586 -5.1731 -0.6487  1.4211 -0.8709  1.6896 -4.0335  0.0067\n",
      "  1.1069 -1.9055 -1.0466 -2.8478 -0.8643 -1.8107 -2.8005 -3.7976 -1.0132\n",
      " -0.1569 -4.3477 -3.2284  2.0389  1.1883 -2.4442 -3.5124 -0.9733  0.4819\n",
      "  2.1227  0.8163 -3.5684 -2.539  -2.5695 -1.3938  0.9293 -3.1422  0.1211\n",
      "  1.1804 -2.0796 -4.2785 -2.7548 -2.0829 -1.4388  0.1428 -3.1631 -5.0098\n",
      " -4.0362 -2.7443 -1.88   -1.5936 -0.0702 -2.5288 -2.3328 -1.544  -0.7295\n",
      "  1.725  -0.4378 -1.5648 -2.0767 -3.3415 -3.1107  0.9975 -3.7706  2.007\n",
      "  0.597  -1.8126  1.7127  2.0019  2.2474 -4.7822 -1.9178 -2.4299  0.4107\n",
      " -3.2284 -3.5194 -2.6912 -3.571   1.9819 -1.7832 -3.3056  2.5535 -1.9896\n",
      " -1.9553  2.2679  1.3074  1.1216 -2.213  -2.9755  1.7406  2.4565 -1.9493\n",
      "  1.1499 -2.2752  1.037  -3.9526 -0.2977 -1.8995 -2.6515 -1.1158 -2.2646\n",
      "  1.4348  0.8696  1.9132 -2.9833 -2.2668  0.3659 -3.4818 -3.5913 -3.2451\n",
      "  2.2437 -1.9431 -2.9818 -0.3015 -4.1984 -3.3738 -3.7729 -0.6125 -1.1183\n",
      " -0.8568  1.9645 -3.8682 -0.7931  1.8453 -2.3234 -0.6106  2.1016 -2.0894\n",
      " -3.3897 -3.2655  1.5109  1.7907  2.7145 -1.5531  1.1186 -2.3059  1.7666\n",
      " -3.1779  1.6174 -1.3837 -3.1151 -2.9518 -2.9224 -3.2592 -3.167  -2.661\n",
      "  0.1955 -2.3114  1.7491 -3.6377 -2.1987  1.1397 -1.3281 -3.3143  0.8515\n",
      " -1.3005 -2.4133 -1.3028 -1.0706 -3.723  -4.3843  2.2486 -0.6827 -2.8251\n",
      " -0.8114 -1.7874  2.0749  1.1145 -0.9308 -2.4336 -5.4505 -1.8257 -4.0769\n",
      " -2.7389  1.3978 -3.335  -1.5977  3.8517 -2.6083 -2.3496 -3.1138  1.2335\n",
      " -2.0674 -2.3958 -1.3893  2.4047 -1.7897 -3.4799 -0.444   0.7991 -1.9286\n",
      " -2.004  -4.4473 -2.4737 -2.8775 -1.1532 -3.0308 -2.6372 -2.2251 -1.0013\n",
      "  2.163  -3.631   1.3912 -2.2169 -1.4073 -3.0208 -1.3217 -1.4799 -2.7981\n",
      "  0.7389 -2.3878  2.5605 -1.1784 -4.616  -3.2379 -3.876   0.6208  2.1081\n",
      "  1.6853 -2.6242 -2.6646 -2.6685 -2.7149 -2.3385 -1.806   1.2763 -3.0067\n",
      "  1.9521 -2.704   1.4354 -3.8401 -0.5995 -2.2728  1.3391  1.7472 -0.556\n",
      "  0.2125  2.2296  1.1933 -4.1733 -3.1329 -3.0716  2.2639 -3.5996 -1.7609\n",
      " -2.9844 -1.7203 -4.7695  0.8347 -3.2081  1.8515 -3.198  -3.2863  1.5931\n",
      " -3.357  -2.6391 -2.128  -1.0114 -0.7405  2.0962  2.4143 -1.5088 -3.3787\n",
      "  0.5667  1.9223 -3.0095 -1.6347 -1.0343 -1.7177  1.7921  0.5884  0.3312\n",
      " -4.6996 -0.9268 -0.1671 -0.3949 -1.9432 -2.0184 -2.1224  2.2014 -0.9168\n",
      " -3.8861 -1.8387  2.0225  1.0561 -2.6858  0.3313 -1.5486 -0.9175 -3.1884\n",
      " -4.82   -3.9126 -3.7437 -2.2679 -3.2204 -1.4855  2.2662 -3.6904 -2.5063\n",
      " -2.1956 -0.08   -2.8565 -3.7891 -1.0717  0.9381  1.9605 -1.9259 -2.2419\n",
      " -1.8773 -2.9251 -1.6227  4.0882 -2.9063 -2.6036 -3.4355 -2.0926 -3.6296\n",
      " -3.9059 -3.7432 -3.0403  0.5618  1.8577 -1.6882 -3.4923 -3.1252 -2.7838\n",
      " -4.0661 -2.0052 -3.6274 -3.25   -3.7798 -3.6354 -1.7554 -3.3568 -2.4947\n",
      " -2.7298 -3.294  -3.7618 -2.3813 -2.1593 -3.1398  1.8439  1.1847 -3.0951\n",
      "  1.5928  1.5308  0.6836  1.4312 -3.1542 -0.8452 -0.7187 -2.1821 -2.6481\n",
      "  2.0336 -2.505   1.6103 -1.0015 -1.423   1.3083  0.3624 -1.6771 -3.4292\n",
      "  2.4472 -3.0296 -2.2248 -3.5125 -2.9761 -2.6329 -3.0708 -2.8295  3.1229\n",
      "  2.2025 -1.3208 -2.7938  3.1806 -1.5755 -4.9094  0.1011 -3.2206  1.2704\n",
      "  1.3922  0.7643 -1.6052 -3.4876 -2.8914 -1.1773 -3.4116 -2.0203 -2.0656\n",
      " -0.7054 -3.5444 -2.3917 -1.0343  1.4847 -2.4211  1.8123  0.449  -1.5361\n",
      "  0.4852 -3.8681 -3.1344 -4.3891  1.0668 -2.6086  2.0656 -3.9934 -3.6524\n",
      " -0.6287 -3.4708 -0.4116  2.0798  1.0022 -1.1885  1.7638  0.6024 -5.0171\n",
      " -3.4426 -3.4609  0.7184  0.8882  2.1564 -2.2357 -0.743  -2.7057 -1.4132\n",
      " -5.9058 -2.8285 -2.0695 -3.7489  1.4315 -0.3132 -1.949  -0.9281  2.2919\n",
      "  1.463   1.5113 -2.6805  0.8718  1.5854 -1.6386 -0.8776 -2.2854 -3.2222\n",
      " -2.1575  1.6537 -3.2109 -1.5868  0.2464 -3.184  -2.577  -2.4543  1.7586\n",
      "  2.0209  1.1947 -2.4547  1.5166  2.5101 -3.3777  2.0586  2.6993 -2.8765\n",
      "  2.2748  1.3499 -2.9607  0.1775 -3.4073  0.1113 -1.8537 -2.5057 -2.3933\n",
      "  0.3536 -0.3054 -3.2124 -3.611   2.0533 -1.0194 -2.8014 -2.8853 -3.1479\n",
      " -1.4016 -3.8077  1.9457 -1.4755  1.7446 -3.2495 -3.4729 -2.1384  2.1575\n",
      " -1.9713 -3.9487 -1.2786 -2.9924 -0.3171 -1.3775  0.5138 -2.0337 -1.4358\n",
      " -1.4503 -4.3497  0.8513 -2.0338 -2.7721  0.3912 -3.1439 -3.4032 -2.3386\n",
      " -2.9552 -3.3336  1.6898 -2.6298 -1.6225 -3.624  -3.122   2.1773 -2.0976\n",
      " -3.2053  1.4875 -2.2199 -2.0778  1.1267  0.1588  1.2696 -2.9155 -3.9461\n",
      " -4.0723  0.7211 -1.5623 -2.2859  1.3547 -3.1128 -3.3058 -4.2754  0.4036\n",
      " -3.1883 -3.2655 -0.6538 -1.6135 -2.9789 -2.6308 -4.3761 -2.45   -1.3098\n",
      " -1.7552 -3.0425 -2.9509 -3.2612 -4.013  -3.5007 -3.0106  1.8333 -4.3143\n",
      "  1.9531 -3.0315  1.9974 -0.4448 -2.2471  0.0953 -1.9733 -2.8838  1.8483\n",
      "  0.8015 -2.7371 -2.0546  2.1796 -2.0752 -2.7568 -3.6134  2.5474  0.9189\n",
      " -2.8173  1.7501 -2.3343 -1.5833 -4.6849 -2.165   1.5997 -3.193   1.524\n",
      " -2.6212 -0.6108 -2.8765 -1.6051 -3.1733  1.3012 -4.5061 -0.9452 -1.7392\n",
      " -0.537  -0.9287 -1.351  -3.198  -2.2129 -2.6258 -4.6081 -1.5109 -3.7166\n",
      " -2.0226 -2.3686  4.7282 -7.2125 -3.3774 -3.5089 -2.9496 -3.6934 -2.7134\n",
      " -0.6582 -2.3284 -3.3195  0.7064 -4.168   2.1543 -2.5867 -3.8443 -4.506\n",
      " -6.6032 -1.3046  0.0066 -1.5647 -2.4798 -3.7451 -2.8909 -3.2396 -3.1838\n",
      "  0.9568  1.7681 -3.0915 -4.1507 -2.7806 -2.1393  1.8756 -2.7687 -4.8342\n",
      "  2.5049 -2.5419 -3.0419  1.4573 -1.7169  1.5827 -2.8322 -2.3081 -2.5994\n",
      " -3.0443  0.4727 -2.6409 -0.8857  1.8275  0.2903 -2.5799 -3.0559 -3.3995\n",
      "  2.0345  2.0849  2.5287 -3.3648 -2.8125  3.9398 -2.586   1.9978 -1.5208\n",
      " -2.2058 -0.9624  1.9862  1.3783 -2.2053 -3.4074 -1.8537 -3.0065 -0.7034\n",
      "  1.4521  1.9583 -3.0295 -4.0986 -3.079   1.8077 -2.2947  1.7976 -0.8502\n",
      " -2.3824  2.2595 -2.5682  1.4755 -3.9389 -2.495  -1.6503 -3.7502  2.4753\n",
      "  1.3929 -4.5229  1.2327  1.9897 -2.7988  1.6012 -3.1419 -4.2255 -2.382\n",
      " -2.9059 -3.2203 -2.9342  1.6434  2.0299  2.4028 -0.5234 -2.3439  2.3535\n",
      "  2.5831  0.0692 -0.9661 -1.8935  0.3089 -1.9646  0.8451  2.3378 -0.2465\n",
      "  1.7731 -3.6805  1.1675 -3.2695  1.7335 -2.2235  1.5879  0.0023  1.611\n",
      "  1.7998 -2.3713 -4.0125 -2.4888 -1.2929  0.9579  1.8242  1.1865 -3.2876\n",
      "  0.9716 -2.682  -3.0634 -1.905  -0.9869  1.1632 -4.1757 -2.7629 -2.9537\n",
      " -2.8016  1.6293 -0.438   1.3343  1.9924 -2.8484 -1.3805  0.6549 -3.4929\n",
      "  1.5325 -2.6073  2.1849 -3.1004 -2.6477  2.768   0.9477  0.2327 -3.0114\n",
      " -2.8659 -3.037  -1.0842  1.488   1.8283  0.9257 -1.8491 -0.5279  0.4436\n",
      " -2.1122 -2.4193  1.9122 -3.076  -2.8723 -3.1847 -2.0806  0.0757  1.7526\n",
      " -2.6324 -2.9376 -0.7512 -0.7323 -0.5136 -1.4304 -3.2844 -2.8433 -3.5691\n",
      " -1.8525 -2.2653  1.1267 -2.7132  3.0244  2.0125 -2.2059  1.6035 -4.0294\n",
      " -3.1874 -2.1594 -1.5717 -2.1814  1.4266 -2.964  -0.8149  1.8265 -4.512\n",
      " -2.4872  1.8317  1.6713 -0.4346  2.6277 -2.7189 -2.9313  0.9746 -1.7216\n",
      " -0.2484  1.0521 -0.2941 -3.5483  2.6027 -1.4865  0.2967  2.2089  1.9055\n",
      "  0.3349 -0.4468  2.2556 -3.3045  0.8938 -4.3838 -2.1659 -2.3079 -2.4887\n",
      " -3.4159 -2.2413 -3.182  -2.1054 -3.9764  1.4957 -3.0339 -0.6481  1.8849\n",
      " -1.9745 -3.7316 -3.1934 -4.1115  2.1039 -2.5887 -3.6031 -1.6956  1.6968\n",
      " -3.4242  1.9476  1.9159 -2.8637 -0.8551 -3.0577  1.9596 -2.6811 -3.1558\n",
      "  3.3738 -4.1146  1.2387  1.8809  1.8374 -2.6586  2.3136 -2.3753 -3.7322\n",
      " -0.7045  1.9289 -2.8998 -1.1959 -1.1064  1.5771 -2.7887 -2.8412 -1.7276\n",
      " -0.7462 -2.3767 -1.3548 -3.4743 -2.4527 -1.0429  2.5231 -1.1296 -2.289\n",
      " -2.2493 -0.682   1.6631  0.9983 -0.0777 -2.0018 -1.4757 -1.4116  1.3345\n",
      " -0.6616 -5.1856  0.8913  1.2746 -1.6196 -0.4061 -1.879   0.4942 -2.6965\n",
      "  0.3518  1.539  -0.5591 -0.4087 -3.9975 -2.991  -0.6231 -2.3615 -0.8401\n",
      " -1.2346 -4.2581 -3.2095 -2.0236  2.6333 -1.4877 -4.8716  2.5275 -3.54\n",
      "  2.8232 -2.8981 -2.7976 -1.69   -2.8279  1.5625 -2.2931  2.1304 -3.277\n",
      "  2.4731 -2.9363 -0.0791 -2.3621 -2.9628 -3.2015 -1.6531 -3.5992 -2.5512\n",
      " -2.5059 -3.1404 -2.5347  1.7031 -3.2944  1.665  -2.0283 -2.9668  0.9907\n",
      " -1.1729 -1.061  -1.89    0.3135  1.8201  0.9841  1.6015  1.6273 -0.1165\n",
      " -1.8276  1.3059 -2.8759 -1.1675 -2.7885 -2.063  -2.2239 -2.4962  1.6667\n",
      " -2.855 ]\n"
     ]
    }
   ],
   "source": [
    "print(non_calibrated_y_pred_test)   #-ve sign indicated correspoing pt in X_test belongs to -ve class and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8781 0.092  0.0086 0.0191 0.2593 0.265  0.1425 0.0064 0.0648 0.8557\n",
      " 0.8918 0.0846 0.0109 0.9102 0.8369 0.0607 0.9641 0.8742 0.861  0.164\n",
      " 0.9373 0.1564 0.0275 0.9649 0.7553 0.0145 0.0777 0.014  0.0829 0.2907\n",
      " 0.0141 0.0917 0.2033 0.9181 0.5505 0.8486 0.9101 0.9104 0.0008 0.2555\n",
      " 0.8491 0.2027 0.89   0.0035 0.4542 0.7863 0.0591 0.167  0.0173 0.2041\n",
      " 0.0666 0.0184 0.0048 0.1734 0.4001 0.0023 0.0104 0.9284 0.8042 0.0294\n",
      " 0.0071 0.1812 0.6126 0.9356 0.7131 0.0066 0.026  0.025  0.1114 0.7433\n",
      " 0.0117 0.4927 0.8026 0.0473 0.0025 0.0195 0.0471 0.1055 0.5    0.0113\n",
      " 0.0009 0.0035 0.0198 0.061  0.0874 0.4286 0.0263 0.0341 0.0929 0.2353\n",
      " 0.8946 0.3134 0.0905 0.0475 0.0089 0.0122 0.7605 0.005  0.9255 0.6488\n",
      " 0.0665 0.893  0.925  0.945  0.0013 0.0582 0.03   0.5896 0.0104 0.007\n",
      " 0.0213 0.0066 0.9231 0.069  0.0094 0.963  0.0531 0.0555 0.9464 0.8283\n",
      " 0.7897 0.0398 0.0146 0.8965 0.958  0.0559 0.7959 0.0367 0.77   0.0039\n",
      " 0.3555 0.0595 0.0224 0.1544 0.0372 0.8515 0.7276 0.9163 0.0144 0.0371\n",
      " 0.5749 0.0074 0.0064 0.0102 0.9448 0.0563 0.0145 0.3543 0.0028 0.0086\n",
      " 0.005  0.2649 0.154  0.2058 0.9214 0.0044 0.2202 0.9089 0.0345 0.2654\n",
      " 0.9338 0.0467 0.0084 0.0099 0.864  0.9027 0.97   0.0918 0.789  0.0353\n",
      " 0.8998 0.0111 0.88   0.1128 0.0121 0.015  0.0156 0.01   0.0113 0.0221\n",
      " 0.5179 0.035  0.8976 0.006  0.0405 0.7937 0.1205 0.0093 0.7227 0.1245\n",
      " 0.0307 0.1242 0.1625 0.0054 0.0022 0.9451 0.2469 0.0178 0.216  0.0686\n",
      " 0.9316 0.7881 0.1899 0.0298 0.0005 0.0654 0.0033 0.02   0.845  0.009\n",
      " 0.0869 0.9934 0.0237 0.0333 0.0121 0.8137 0.048  0.0314 0.112  0.9551\n",
      " 0.0684 0.0074 0.3116 0.7083 0.0574 0.0521 0.002  0.0283 0.0166 0.1479\n",
      " 0.0135 0.0228 0.0392 0.1757 0.9388 0.0061 0.8439 0.0396 0.1096 0.0137\n",
      " 0.1214 0.1004 0.0185 0.6912 0.0317 0.9633 0.1437 0.0016 0.0103 0.0044\n",
      " 0.6562 0.9344 0.8894 0.0232 0.022  0.0219 0.0206 0.0338 0.067  0.8223\n",
      " 0.014  0.9202 0.0209 0.8516 0.0046 0.2684 0.0368 0.8344 0.8974 0.2801\n",
      " 0.5236 0.9438 0.8053 0.0029 0.0118 0.0128 0.9462 0.0063 0.0709 0.0144\n",
      " 0.0746 0.0013 0.7181 0.0107 0.9096 0.0108 0.0096 0.8765 0.0088 0.0228\n",
      " 0.0444 0.1737 0.2326 0.9334 0.9556 0.0969 0.0085 0.6395 0.9172 0.0139\n",
      " 0.083  0.1693 0.0749 0.9028 0.6462 0.5634 0.0014 0.1907 0.3969 0.326\n",
      " 0.0563 0.0512 0.0448 0.9417 0.1928 0.0043 0.0643 0.9269 0.7746 0.0214\n",
      " 0.5634 0.0923 0.1927 0.011  0.0012 0.0042 0.0052 0.0371 0.0105 0.0997\n",
      " 0.9463 0.0056 0.0271 0.0407 0.4253 0.0171 0.0049 0.1623 0.7455 0.921\n",
      " 0.0576 0.0383 0.0612 0.0156 0.0843 0.9952 0.016  0.0239 0.0079 0.0465\n",
      " 0.0061 0.0042 0.0052 0.0134 0.638  0.9103 0.0777 0.0073 0.0119 0.0188\n",
      " 0.0034 0.052  0.0061 0.0101 0.005  0.006  0.0714 0.0088 0.0275 0.0202\n",
      " 0.0095 0.0051 0.032  0.0427 0.0117 0.9088 0.8035 0.0124 0.8765 0.8671\n",
      " 0.675  0.8508 0.0115 0.2084 0.238  0.0414 0.0225 0.9279 0.0272 0.879\n",
      " 0.1757 0.1076 0.8285 0.5737 0.0788 0.0079 0.9575 0.0136 0.0392 0.0071\n",
      " 0.0146 0.023  0.0128 0.0177 0.9825 0.9418 0.1216 0.0186 0.9838 0.0893\n",
      " 0.0011 0.486  0.0105 0.8211 0.844  0.6985 0.0861 0.0073 0.0163 0.1438\n",
      " 0.0081 0.051  0.0482 0.2412 0.0068 0.0315 0.1693 0.8598 0.0303 0.9052\n",
      " 0.602  0.0938 0.6137 0.0044 0.0118 0.0022 0.7771 0.0237 0.9308 0.0037\n",
      " 0.0059 0.2607 0.0075 0.321  0.932  0.7616 0.142  0.8994 0.6505 0.0009\n",
      " 0.0078 0.0076 0.6853 0.7325 0.9383 0.0386 0.232  0.0209 0.1089 0.0003\n",
      " 0.0177 0.0479 0.0052 0.8509 0.3507 0.0559 0.1905 0.9481 0.8562 0.8641\n",
      " 0.0216 0.7282 0.8754 0.0826 0.2012 0.0362 0.0105 0.0428 0.8851 0.0106\n",
      " 0.0881 0.535  0.011  0.0247 0.029  0.8988 0.9268 0.8056 0.029  0.8649\n",
      " 0.9608 0.0085 0.9302 0.9694 0.0166 0.9469 0.8364 0.0149 0.5118 0.0082\n",
      " 0.4894 0.0631 0.0271 0.0315 0.5708 0.3531 0.0106 0.0062 0.9297 0.1722\n",
      " 0.0184 0.0164 0.0116 0.1104 0.0048 0.9196 0.101  0.897  0.0101 0.0075\n",
      " 0.0438 0.9384 0.0543 0.004  0.1278 0.0143 0.3495 0.1136 0.6228 0.0502\n",
      " 0.1059 0.1041 0.0023 0.7226 0.0502 0.0191 0.5832 0.0116 0.0082 0.0338\n",
      " 0.015  0.009  0.89   0.0231 0.0843 0.0061 0.012  0.9399 0.0462 0.0107\n",
      " 0.8603 0.0394 0.0474 0.7908 0.5054 0.821  0.0158 0.004  0.0033 0.6861\n",
      " 0.0908 0.0362 0.8373 0.0121 0.0094 0.0025 0.5873 0.011  0.0099 0.2542\n",
      " 0.0852 0.0145 0.023  0.0022 0.0292 0.1232 0.0715 0.0133 0.0151 0.01\n",
      " 0.0036 0.0072 0.0139 0.9076 0.0024 0.9203 0.0135 0.9246 0.3113 0.0381\n",
      " 0.484  0.0542 0.0165 0.9093 0.709  0.02   0.0488 0.9401 0.0476 0.0195\n",
      " 0.0062 0.9627 0.7406 0.018  0.8977 0.034  0.0885 0.0015 0.0424 0.8775\n",
      " 0.0109 0.8661 0.0233 0.2654 0.0166 0.0861 0.0112 0.8272 0.0019 0.1869\n",
      " 0.0729 0.2853 0.1903 0.1173 0.0108 0.0398 0.0232 0.0016 0.0967 0.0054\n",
      " 0.0509 0.0325 0.998  0.     0.0085 0.0071 0.0151 0.0056 0.0206 0.2531\n",
      " 0.0342 0.0092 0.6817 0.0029 0.9381 0.0244 0.0046 0.0019 0.0001 0.1239\n",
      " 0.4542 0.0905 0.0281 0.0052 0.0163 0.0102 0.011  0.7503 0.8999 0.0125\n",
      " 0.003  0.0189 0.0438 0.9123 0.0192 0.0012 0.9605 0.0259 0.0133 0.8553\n",
      " 0.075  0.875  0.0176 0.0352 0.024  0.0133 0.6097 0.0227 0.1995 0.9069\n",
      " 0.5497 0.0246 0.0131 0.0083 0.928  0.9324 0.9617 0.0087 0.0181 0.9941\n",
      " 0.0244 0.9246 0.0955 0.0402 0.1834 0.9235 0.8415 0.0402 0.0082 0.0631\n",
      " 0.014  0.2417 0.8544 0.9208 0.0136 0.0032 0.0127 0.9047 0.0358 0.9035\n",
      " 0.2072 0.0319 0.9459 0.025  0.8583 0.004  0.0275 0.0814 0.0052 0.959\n",
      " 0.8442 0.0018 0.8135 0.9239 0.0184 0.8777 0.0117 0.0027 0.0319 0.016\n",
      " 0.0105 0.0154 0.8837 0.9276 0.955  0.289  0.0336 0.952  0.9644 0.4752\n",
      " 0.1827 0.06   0.5559 0.0548 0.721  0.951  0.3715 0.9005 0.0057 0.7998\n",
      " 0.0098 0.8956 0.0393 0.8758 0.4527 0.8791 0.9037 0.0324 0.0036 0.0278\n",
      " 0.1256 0.7506 0.9066 0.8039 0.0096 0.754  0.0215 0.013  0.0591 0.1785\n",
      " 0.7988 0.0029 0.0193 0.015  0.0184 0.8817 0.3133 0.8335 0.9241 0.0173\n",
      " 0.1132 0.6665 0.0073 0.8674 0.0238 0.9405 0.0123 0.0225 0.972  0.748\n",
      " 0.5304 0.0139 0.0169 0.0134 0.16   0.8603 0.907  0.7424 0.0635 0.2878\n",
      " 0.6003 0.0453 0.0304 0.9162 0.0127 0.0167 0.011  0.0472 0.4774 0.898\n",
      " 0.023  0.0153 0.2301 0.2346 0.2918 0.1066 0.0097 0.0174 0.0066 0.0632\n",
      " 0.0372 0.7908 0.0206 0.98   0.926  0.0402 0.878  0.0035 0.011  0.0427\n",
      " 0.0897 0.0415 0.8501 0.0148 0.2152 0.9068 0.0019 0.0278 0.9074 0.8875\n",
      " 0.3143 0.9664 0.0205 0.0155 0.7548 0.0745 0.3709 0.7737 0.3566 0.0068\n",
      " 0.9653 0.0996 0.5518 0.9423 0.9155 0.5646 0.3108 0.9456 0.0094 0.734\n",
      " 0.0022 0.0423 0.0352 0.0278 0.0081 0.0384 0.0111 0.0457 0.0038 0.8616\n",
      " 0.0135 0.2557 0.9133 0.0541 0.0053 0.0109 0.0032 0.934  0.0243 0.0063\n",
      " 0.077  0.8909 0.008  0.9198 0.9165 0.0169 0.2061 0.0131 0.9209 0.0215\n",
      " 0.0115 0.9875 0.0032 0.8147 0.9129 0.9081 0.0222 0.9495 0.0322 0.0053\n",
      " 0.2415 0.9179 0.0161 0.1408 0.1561 0.8742 0.0187 0.0174 0.074  0.2313\n",
      " 0.0322 0.1167 0.0075 0.0291 0.1677 0.9615 0.152  0.0361 0.038  0.2471\n",
      " 0.8864 0.7607 0.4261 0.0523 0.1009 0.1091 0.8335 0.2522 0.0007 0.7334\n",
      " 0.822  0.0846 0.3227 0.0611 0.6166 0.0211 0.5702 0.8684 0.2792 0.3219\n",
      " 0.0037 0.0143 0.2622 0.0328 0.2095 0.1346 0.0026 0.0107 0.0508 0.9666\n",
      " 0.0995 0.0011 0.9617 0.0069 0.974  0.0162 0.0185 0.0775 0.0177 0.872\n",
      " 0.0359 0.9362 0.0097 0.9589 0.0154 0.4257 0.0328 0.0148 0.0108 0.0812\n",
      " 0.0063 0.0256 0.0271 0.0117 0.0261 0.8917 0.0095 0.8867 0.0505 0.0147\n",
      " 0.7588 0.1446 0.1643 0.0603 0.5575 0.9061 0.7571 0.8778 0.8815 0.4133\n",
      " 0.0652 0.8281 0.0166 0.1455 0.0187 0.0483 0.0392 0.0275 0.8869 0.0171]\n"
     ]
    }
   ],
   "source": [
    "print(calibrated_y_pred_test)    #in the range between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "- Calibrated porbability output of models are neccessary when we want to classify something with how sure we are about its class.(ex, prob=0.25 in cancer classification, if the model says that prob of having cancer for a person is 0.25, then in empirical probibily of this person having cancer also very close to 25%) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
